%
% File acl2018.tex
%
%% Based on the style files for ACL-2017, with some changes, which were, in turn,
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2018}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{url}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{Paper plan}

%\author{First Author \\
%  Affiliation / Address line 1 \\
%  Affiliation / Address line 2 \\
%  Affiliation / Address line 3 \\
%  {\tt email@domain} \\\And
%  Second Author \\
%  Affiliation / Address line 1 \\
%  Affiliation / Address line 2 \\
%  Affiliation / Address line 3 \\
%  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}

\end{abstract}

\section{Possible paper outline}

Artificial agents that need follow human instructions need to be robust to 
changing conditions in the environment. In this sense, they need to be able to 
\emph{generalize} to new situations and \emph{adapt} to new speakers. In this 
paper we seek to address these two challenges with a study based on the 
SHRDLURN environment \cite{Wang:etal:2016}. 

\paragraph{Experiment 1} We first train a system to execute human instructions 
in the SRHDLURN environment. The system receives as input a configuration of 
colored  blocks organized into piles and a natural language instruction for 
what to do with them and has to produce the new resulting configuration. To 
this end, we train an end-to-end system on a large-scale artificial dataset 
especially built for this purpose. We evaluate this system on three 
generalization tests:

\begin{itemize}
	\item Executing already seen instructions on new configurations
	\item Executing new instructions on already seen configurations
	\item Executing new instructions on new configurations.
\end{itemize}

(The baseline system from the original paper should also be tested here. I 
think it should be ok to loose against it. After all, it has hardcoded 
compositionality because it uses a compositional programming language to 
produce the new target configurations.). We also introduce the AND composition 
operator, which can be used to create sequences of joint operations and test it 
on a similar setup.

\paragraph{Experiment 2}

This is all nice, but how can it all be useful for a new user of the system? 
Let's imagine a new user comes in and wants to use the system but doesn't speak 
in the same way as the first user that trained it. How can this person use this 
system? To explore this question we create a new artificial dataset where we 
introduce variations in how the speaker expresses herself, but controlling for 
these variations (we need to decide what to explore here). In the extreme, the 
speaker uses an entirely new language. To model new vocabulary words, we have 
to create new randomly-initialized embeddings. This model, learned with 
gradient descent on new utterances will make the fist baseline. Here's another 
proposal for a second fast-adaptation model. The idea would be to try to 
``copy'' from the previous embeddings any possible useful information. To do 
this, we could allow to enrich the randomly initialized embeddings with a 
weighted sum of the previously learned words. Formally, let 
$E$ be the original embedding matrix and $e_w$ a new randomly initialized 
embedding, we will construct a ``fast'' embedding $\hat{e}_w$ that will be 
forwarded to upper layers as 

$$\hat{e}_w = \lambda_w e_w + (1-\lambda_w) \sum_j{\alpha_j E_j}$$

The $\alpha_j$ weights could be simply computed as $\operatorname{softmax}(e_w 
^\top E)$ or something along those lines and $\lambda_w$ is just an extra 
parameter.

 We test the fast-adaptation model for this task (see below) and compare to:
\begin{itemize}
	\item Original baseline
	\item Fine-tunning the original model (adding random embeddings)
\end{itemize}

\paragraph{Experiment 3}

I think it would then be interesting to see how we fare on real the real 
dataset. Again, we should compare against the original baseline and 
fine-tunning.


%\input{contents/introduction.tex}
%\input{contents/related-work.tex}
%\input{contents/model.tex}
%\input{contents/experiments.tex}
%\input{contents/conclusions.tex}


\end{document}
